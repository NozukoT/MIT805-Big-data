# MIT 805 project

# Introduction

The DonorsChoose.org dataset contains historical data collected over 5 years historical for batch analysis, but with a potential for new streaming capabilities as a result of using more advanced cutting-edge technologies. analysis. The DonorsChoose.org version 8 dataset constitutes over 1 million project related for public schools donation interventions organized by a US based non profit organization (NPO).  The data is publicly available on Kaggle data science for good challenge website, safely published without contravening privacy and confidentiality protection of individual data. The data owners DonorsChoose.org has provided anonymized cumulated data over 5 years as a project data to assist public school classroom needs to be matched with the donors motivated to provide more. All technical aspects including software development for data platforms requirements must be coherent to big data categorization process focusing on collection, processing, visualization. This project will employ big data platforms MapReduce and Hadoop for processing, in Python for analysis machine learning algorithms while employing Plotly for visualisation and Streamlit for deployment.

# Project Problem Statement

Can machine learning techniques predict accurately the matching of donors to school projects of interest resulting in an analytical recommender system that also considers a hybrid approach for streaming and batch data?

## Sub-objectives

* To determine if the location for schools matters in receiving most and full funded donations?
* Which periods of the year are more likely to have made more active projects?
* On average what is the donation amount per month in comparison to the needed donation per school?
* Classication of project success by project resource category and correlation with project leader according to teacher, professional development
  and student?

## Availabilty of data for the analysis

The data set is made of multiple data sources from the DonorsChoose.org comprising a total size of 3.87GB from a variety of 6 datasets of CSV type:
* Donations dataset with 7 variables of 583.03MB size. 
* Donors dataset with 5 variables 118.24 MB size. 
* Projects dataset with 18 variables of 2.36GB size.
* Resources dataset with 5 variables of 781.39MB size. 
* Schools dataset of with 9 variables of 9.21MB size. 
* Teachers dataset with 3 variables of 18.64 MB size.

## Age

The projects dataset was released in January 2018, for data posted over a 6 year period from 2013 to 2018 year. There has been no updated information after 2018 on the Kaggle website.

## Current Version:

Version 8. The last update was on 2018-05-14 when it was created 2018-04-26.

## Architecture in Big Data

Big data characteristics coherently mapped into big data categories demonstrate computational tools for software architect to capture, curate, store, search, share, transfer, analyze and visualize insights. This includes understanding of existing technology tools, data platforms in Hadoop while usibg Python programming language and Plotly, Tableaue etc for visualisation including Streamlit for deployment.

## Processing of data

The software development employs MapReduce Hadoop to crunch large data on comfortable processing power, with processing distributed either
on one or across multiple computers at a high speed. Hadoop is versatile to be deployed on streaming data and compatible with Java, Python for machine
learning techniques.

# Visualization:

For this framework that allows the big data visualization and providing data insights, tools such as Plotly, Tableau, Streamlit will be utilized for bringing and deployment of insights for use.

# Conclusion

he NGO was initiated early 2000 with uptake rising from year 2012 as projects gained momentum.  This dataset is stored as a batch file and may be utilised for analysis utilising characteristics of big data management and implementation processes.
Due to increased and complex datasets requiring computational tools competencies for batch and input and output capabilities,there is a need to consider software architecture for tools that can facilitate without bottle necks the batching and streaming demands of big data. This implies software development that no longer necessitate conventional and traditional management and processing of big data are favourable, favouring computational tools that can handle high Volume, Velocity, Veracity and Variety datasets. 

The chosen software architect for batch processing and analysis for the DonorsChoose.org project follows the factors of big characteristics that is, data collection, processing, analysis, visualisation and deployment of results.  

# References
[1] Software engineering for scientific big data analysis, available online on:
https://www.researchgate.net/publication/333326758Softwareengineeringf orscientificbigdataanalysis:Lastaccessed12September2020:
[2] Data Science for good: DonorsChoose.org available online on: https://www.kaggle.com/donorschoose/io ,last assessed 14 September 2020.
